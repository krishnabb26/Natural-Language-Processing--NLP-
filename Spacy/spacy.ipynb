{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc0e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5b9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002b0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('Dr.strange loves indian foods. Hulk came india yesterday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5e13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.strange loves indian foods.\n",
      "Hulk came india yesterday\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "    \n",
    "# This is basically sentence tokenization in spacy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d798188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.strange\n",
      "loves\n",
      "indian\n",
      "foods\n",
      ".\n",
      "Hulk\n",
      "came\n",
      "india\n",
      "yesterday\n"
     ]
    }
   ],
   "source": [
    "# If you want to seperate out individual words from this sentences\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)\n",
    "        \n",
    "# this is word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617b38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")     # en stands for english\n",
    "\n",
    "doc = nlp(\"Dr. strange loves chennai. He spent 500$ for food.\")\n",
    "\n",
    "# spacy language models for more language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16466b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "strange\n",
      "loves\n",
      "chennai\n",
      ".\n",
      "He\n",
      "spent\n",
      "500\n",
      "$\n",
      "for\n",
      "food\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)\n",
    "    \n",
    "# It as done already word tokenizattion    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3494b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "059749c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(spacy.lang.en.English, spacy.tokens.doc.Doc, spacy.tokens.token.Token)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp), type(doc), type(token)\n",
    "\n",
    "# if you look type of object\n",
    "# for nlp is object of english lang\n",
    "# doc is object of doc\n",
    "# token is object of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164fbd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr. strange loves chennai."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[0:5]\n",
    "span\n",
    "# SPAN - span means a sub string from that string \n",
    "# if you see type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c470ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(span)\n",
    "\n",
    "# you get object span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d9f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tony gave twenty $ to Peter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7752c4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0\n",
    "\n",
    "# this as certain attributes if see in dir on any py variable\n",
    "# we get all the method, the class token as all these methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938b28c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1e6481f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha, token0.like_num\n",
    "\n",
    "# its true because its alphabet if you num it will be false beacse its alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51712fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twenty'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[2]\n",
    "token2.text\n",
    "\n",
    "# if you give .text it will give you the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d46ec93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69cf9057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3 = doc[3]\n",
    "token3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9c5384c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ba7b7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony --> index: 0\n",
      " is_alpha: True\n",
      " is_punct: False\n",
      " like_num: False\n",
      " is_currency: False\n",
      "\n",
      "gave --> index: 1\n",
      " is_alpha: True\n",
      " is_punct: False\n",
      " like_num: False\n",
      " is_currency: False\n",
      "\n",
      "twenty --> index: 2\n",
      " is_alpha: True\n",
      " is_punct: False\n",
      " like_num: True\n",
      " is_currency: False\n",
      "\n",
      "$ --> index: 3\n",
      " is_alpha: False\n",
      " is_punct: False\n",
      " like_num: False\n",
      " is_currency: True\n",
      "\n",
      "to --> index: 4\n",
      " is_alpha: True\n",
      " is_punct: False\n",
      " like_num: False\n",
      " is_currency: False\n",
      "\n",
      "Peter --> index: 5\n",
      " is_alpha: True\n",
      " is_punct: False\n",
      " like_num: False\n",
      " is_currency: False\n",
      "\n",
      ". --> index: 6\n",
      " is_alpha: False\n",
      " is_punct: True\n",
      " like_num: False\n",
      " is_currency: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token} --> index: {token.i}\\n is_alpha: {token.is_alpha}\\n is_punct: {token.is_punct}\\n like_num: {token.like_num}\\n is_currency: {token.is_currency}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59c58f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_tamil = spacy.blank(\"ta\")\n",
    "\n",
    "doc = nlp_tamil('நேற்று நான் அனுப்பிய 200 $ உங்களுக்கு கிடைத்தது என்று நம்புகிறேன்.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bec4fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "நேற்று False False\n",
      "நான் False False\n",
      "அனுப்பிய False False\n",
      "200 False True\n",
      "$ True False\n",
      "உங்களுக்கு False False\n",
      "கிடைத்தது False False\n",
      "என்று False False\n",
      "நம்புகிறேன் False False\n",
      ". False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.is_currency, token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b95e604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['giveme', 'double', 'cheese', 'pizza', '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('giveme double cheese pizza.')\n",
    "\n",
    "tokens = [token.text for token in doc]  \n",
    "tokens\n",
    "\n",
    "# giveme is actually two words give me, you want 2 seperate token\n",
    "# you can customize spacy here\n",
    "# Note : cant change the words but can split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b169056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['give', 'me', 'double', 'cheese', 'pizza', '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"giveme\", [{ORTH: 'give'},{ORTH: 'me'}])\n",
    "\n",
    "\n",
    "# simply when ever you see giveme as like this give that as two token give me\n",
    "\n",
    "doc = nlp('giveme double cheese pizza.')\n",
    "\n",
    "tokens = [token.text for token in doc]  \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98ed98d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3093670136.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\KRISHNA\\AppData\\Local\\Temp\\ipykernel_7984\\3093670136.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print(token)''''\u001b[0m\n\u001b[1;37m                    \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# ''''doc = nlp(\"Dr. strange loves chennai. He spent 500$ for food.\")\n",
    "\n",
    "# for token in doc.sents:         #sents is sentence sentence_tokenizer\n",
    "#     print(token)''''\n",
    "    \n",
    "# if you run its show error that the nlp pipeline is blank  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d57bc907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n",
    "# it is blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56ef2127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x24be9852c80>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')   # add sentencizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7706443b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you see the pipline\n",
    "\n",
    "nlp.pipe_names\n",
    "\n",
    "# now my pipeline as this sentencizer component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c3e8913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. strange loves chennai.\n",
      "He spent 500$ for food.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. strange loves chennai. He spent 500$ for food.\")\n",
    "\n",
    "for token in doc.sents:       \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccc16d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "doc = nlp(text)\n",
    "# Lets collect only the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59b0ec22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "websites = [token.text for token in doc if token.like_url]\n",
    "websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04c1b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all money transaction from below sentence along with currency. \n",
    "\n",
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "doc = nlp(transactions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c159cb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token.text, doc[token.i+1].text)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510bb92",
   "metadata": {},
   "source": [
    "# Language Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f0637c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy documentation you can download pretrained pipeline for for different languages\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "# use it to pip download the trained pipeline for english language or in cmd prompt too.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c41d6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of creating blank pipeline \n",
    "\n",
    "nlp_pipeline = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f59a5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now if you see the pipe line you can see all those components\n",
    "# when you load trained pipeline you get some inbuild features\n",
    "\n",
    "nlp_pipeline.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "904afd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  PROPN  |  Captain\n",
      "america  |  PROPN  |  america\n",
      "ate  |  VERB  |  eat\n",
      "100  |  NUM  |  100\n",
      "$  |  NUM  |  $\n",
      "of  |  ADP  |  of\n",
      "healthy  |  ADJ  |  healthy\n",
      "meal  |  NOUN  |  meal\n",
      ".  |  PUNCT  |  .\n",
      "then  |  ADV  |  then\n",
      "he  |  PRON  |  he\n",
      "said  |  VERB  |  say\n",
      "i  |  PRON  |  I\n",
      "can  |  AUX  |  can\n",
      "do  |  VERB  |  do\n",
      "this  |  PRON  |  this\n",
      "all  |  DET  |  all\n",
      "day  |  NOUN  |  day\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_pipeline(\"Captain america ate 100$ of healthy meal. then he said i can do this all day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \",token.lemma_)\n",
    "    \n",
    "#Pos(parts of speech) -- > There are eight pos in the English language: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection\n",
    "# lemma --> base word you can see for ate the base word is eat\n",
    "\n",
    "# i got pos because of the tagger component in pipeline and lemmatizer gives lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648789b9",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9b0bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# for ner \n",
    "\n",
    "doc = nlp_pipeline(\"Tesla Inc as acquired twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:          #ent --> entities\n",
    "    print(ent.text, \" | \",ent.label_, \" | \", spacy.explain(ent.label_))\n",
    "    \n",
    "    \n",
    "# it says tels is organization and 45 billion is money, it is recoganizing the entities\n",
    "# the spacy.explain it can explain or show you short description on label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbc3dd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " as acquired twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To show this entities in a nice display or presentation way youcan use this module \"displacy\"\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style = 'ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3607dc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see all the entities\n",
    "nlp_pipeline.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4137fef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# it is not taking twitter as ORG if i use Twitter, Inc it recoganize as org\n",
    "# we can specify the company by span method\n",
    "\n",
    "doc = nlp_pipeline(\"tesl as acquired twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:          \n",
    "    print(ent.text, \" | \",ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c76ab932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 0, 1, label = 'ORG')\n",
    "s2 = Span(doc, 3, 4, label = 'ORG')\n",
    "\n",
    "doc.set_ents([s1, s2], default = 'unmodified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f599bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesl  |  ORG\n",
      "twitter  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:          \n",
    "    print(ent.text, \" | \",ent.label_,) \n",
    "    \n",
    "# now it is recoganizing it as ORG(organization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c28ed5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tesl\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " as acquired \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    twitter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = 'ent')\n",
    "\n",
    "# now it is showing tesla and twitter as org with out Inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3047e599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for customizing the pipeline with the blank pipeline for some task\n",
    "\n",
    "source_pipeline = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create blank pipeline\n",
    "nlp_ner = spacy.blank(\"en\")\n",
    "\n",
    "# add ner from source pipeline\n",
    "nlp_ner.add_pipe(\"ner\", source = source_pipeline)\n",
    "\n",
    "nlp_ner.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d11f0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13%  |  PERCENT  |  Percentage, including \"%\"\n",
      "Tesla  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "79%  |  PERCENT  |  Percentage, including \"%\"\n",
      "Twitter, Inc.  |  ORG  |  Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_ner(\"Elon musk has a 13% ownership stake in Tesla and a 79% stake in Twitter, Inc.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \",ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f77514",
   "metadata": {},
   "source": [
    "# stemming and lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b212ca1",
   "metadata": {},
   "source": [
    "- For stemming use nltk because spacy dosn't have support for stemming their is an article were spacy author have mentioned why they don't support stemming because lemmatization is more suffesticated, more correct they just have lematization support.\n",
    "- nltk as both Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82e2d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "eats | eat\n",
      "adjustable | adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meeting\n",
      "better | well\n"
     ]
    }
   ],
   "source": [
    "nlp_l = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp_l('eating eat ate eats adjustable rafting ability meeting better')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a577c920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_l.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d309f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | bro\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brah\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      ". | .\n"
     ]
    }
   ],
   "source": [
    "# attribute_ruler  in pipeline assign attribute to an particular token you want to customize, what i mean here is\n",
    "\n",
    "doc = nlp_l(\"Bro, you wanna go? Brah, don't say no.\")  #[both \"bro & brah represent Brother(base word)\"]\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.lemma_)\n",
    "    \n",
    "# the lang model dosn't understand bcz this is slang the way the tone or speaking of persons    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c258b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bro, 'bro')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you look at 1st token\n",
    "doc[0], doc[0].lemma_\n",
    "\n",
    "# lemma is bro, because it dosn't understand the slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee09fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      ". | .\n"
     ]
    }
   ],
   "source": [
    "# we can customize it we know bro and  brah are brother \n",
    "\n",
    "ar = nlp_l.get_pipe(\"attribute_ruler\") #get_pipe will give you that particular component from the pipeline\n",
    "\n",
    "# we can customize it by adding the custome rule\n",
    "ar.add([[{\"TEXT\": \"Bro\"}], [{\"TEXT\": \"Brah\"}]], {\"LEMMA\": \"Brother\"})\n",
    "# i want lemma as brother for few words (bro,brah)\n",
    "# attribute is LEMMA\n",
    "\n",
    "doc = nlp_l(\"Bro, you wanna go? Brah, don't say no.\")  \n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b87fe47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bro, 'Brother')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you see the base word\n",
    "doc[0], doc[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7c76051",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Latha is very multi talented girl.She is good at many skills like dancing, running, singing, playing.She also likes eating Pav Bhagi. she has a habit of fishing and swimming too.Besides all this, she is a wonderful at cooking too.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "343251c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_l(text)\n",
    "\n",
    "\n",
    "# getting the base form for each token using spacy 'lemma_'\n",
    "base_words = []\n",
    "\n",
    "for token in doc:\n",
    "    words = token.lemma_\n",
    "    base_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6130417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Latha be very multi talented girl . she be good at many skill like dancing , running , singing , play . she also like eat Pav Bhagi . she have a habit of fishing and swim too . besides all this , she be a wonderful at cook too .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joining all words in a list into string using 'join()'\n",
    "\n",
    "base_words_text = \" \".join(base_words)\n",
    "base_words_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26377344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see by stemming using nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#  Word tokenizing\n",
    "word_tokenize = nltk.word_tokenize(text)\n",
    "\n",
    "base_word = []\n",
    "for token in word_tokenize:\n",
    "    word = stemmer.stem(token)\n",
    "    base_word.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec1cce0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'latha is veri multi talent girl.sh is good at mani skill like danc , run , sing , playing.sh also like eat pav bhagi . she ha a habit of fish and swim too.besid all thi , she is a wonder at cook too .'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_word_text = \" \".join(base_word)\n",
    "base_word_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068b069",
   "metadata": {},
   "source": [
    "# Part of speech POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ea41b",
   "metadata": {},
   "source": [
    "- NOUN         --> person, Place, Thing, Idea.  \n",
    "- VERB         --> Action or Presence.  \n",
    "- PRONOUN      --> it is a substitute of an Noun.  [He, She, Our, You, They, I]\n",
    "- ADJECTIVE    --> Describes the Noun. Add meaning to it. \n",
    "- ADVERB       --> Describes Verbs, Adjective or Adverb.  (They add little more meaning to activity.)\n",
    "- INTERJECTION --> They represent strong emotion or an expression.  [WOW!, HEY!!]\n",
    "- CONJUCTION   --> Connect words or group of words.  [but, and, or]\n",
    "- PREPOSITION  --> Links a Noun to another Noun.  [in, at, on] e.g:- (Thor{NOUN} is on{PREPOSITION} the bus{NOUN})\n",
    "                  - This preeposition are very powerfull they can change the meaning of the sentence.\n",
    "                  - they also vary based on level [Preposition, Postposition and Adposition]\n",
    "\n",
    "1. These POS Tag you see this 8 are the fundamental in Eng. lang Grammer.\n",
    "2. Their are couple more like Numeral, article or determiner  \n",
    "  \n",
    "  You can check [POS_link](https://v2.spacy.io/api/annotation) hear for the complete list of pos categories in spacy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d80cd205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  interjection  |  UH  |  interjection\n",
      "!  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
      "Dr.  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "Strange  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "made  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
      "234  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "million  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "$  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "on  |  ADP  |  adposition  |  IN  |  conjunction, subordinating or preposition\n",
      "the  |  DET  |  determiner  |  DT  |  determiner\n",
      "first  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "day  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp('Wow! Dr. Strange made 234 million $ on the first day')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \",spacy.explain(token.pos_),\n",
    "         \" | \",token.tag_, \" | \",spacy.explain(token.tag_))\n",
    "    \n",
    "    # Tag as more more detail than Pos contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12cf5464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quits  |  VERB  |  VBZ  |  verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('He quits the job.')\n",
    "\n",
    "\n",
    "print(doc[1].text, \" | \", doc[1].pos_, \" | \",doc[1].tag_,\" | \",spacy.explain(doc[1].tag_))\n",
    "\n",
    "#it says it is \"verb, 3rd person singular present\"\n",
    "# lets try to remove s from quit's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a54d45ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quit  |  VERB  |  VBD  |  verb, past tense\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('He quit the job.')\n",
    "\n",
    "\n",
    "print(doc[1].text, \" | \", doc[1].pos_, \" | \",doc[1].tag_,\" | \",spacy.explain(doc[1].tag_))\n",
    "\n",
    "# now it nows this is a past tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b750654",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·         Revenue was $51.7 billion and increased 20%\n",
    "·         Operating income was $22.2 billion and increased 24%\n",
    "·         Net income was $18.8 billion and increased 21%\n",
    "·         Diluted earnings per share was $2.48 and increased 22%\n",
    "“Digital technology etc. is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
    "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cl\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "62ea063e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft  |  PROPN  |  proper noun\n",
      "Corp.  |  PROPN  |  proper noun\n",
      "today  |  NOUN  |  noun\n",
      "announced  |  VERB  |  verb\n",
      "the  |  DET  |  determiner\n",
      "following  |  VERB  |  verb\n",
      "results  |  NOUN  |  noun\n",
      "for  |  ADP  |  adposition\n",
      "the  |  DET  |  determiner\n",
      "quarter  |  NOUN  |  noun\n",
      "ended  |  VERB  |  verb\n",
      "December  |  PROPN  |  proper noun\n",
      "31  |  NUM  |  numeral\n",
      ",  |  PUNCT  |  punctuation\n",
      "2021  |  NUM  |  numeral\n",
      ",  |  PUNCT  |  punctuation\n",
      "as  |  SCONJ  |  subordinating conjunction\n",
      "compared  |  VERB  |  verb\n",
      "to  |  ADP  |  adposition\n",
      "the  |  DET  |  determiner\n",
      "corresponding  |  ADJ  |  adjective\n",
      "period  |  NOUN  |  noun\n",
      "of  |  ADP  |  adposition\n",
      "last  |  ADJ  |  adjective\n",
      "fiscal  |  ADJ  |  adjective\n",
      "year  |  NOUN  |  noun\n",
      ":  |  PUNCT  |  punctuation\n",
      "\n",
      "\n",
      "  |  SPACE  |  space\n",
      "·  |  PUNCT  |  punctuation\n",
      "          |  SPACE  |  space\n",
      "Revenue  |  NOUN  |  noun\n",
      "was  |  AUX  |  auxiliary\n",
      "$  |  SYM  |  symbol\n",
      "51.7  |  NUM  |  numeral\n",
      "billion  |  NUM  |  numeral\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "increased  |  VERB  |  verb\n",
      "20  |  NUM  |  numeral\n",
      "%  |  NOUN  |  noun\n",
      "\n",
      "  |  SPACE  |  space\n",
      "·  |  PUNCT  |  punctuation\n",
      "          |  SPACE  |  space\n",
      "Operating  |  VERB  |  verb\n",
      "income  |  NOUN  |  noun\n",
      "was  |  AUX  |  auxiliary\n",
      "$  |  SYM  |  symbol\n",
      "22.2  |  NUM  |  numeral\n",
      "billion  |  NUM  |  numeral\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "increased  |  VERB  |  verb\n",
      "24  |  NUM  |  numeral\n",
      "%  |  NOUN  |  noun\n",
      "\n",
      "  |  SPACE  |  space\n",
      "·  |  PUNCT  |  punctuation\n",
      "          |  SPACE  |  space\n",
      "Net  |  ADJ  |  adjective\n",
      "income  |  NOUN  |  noun\n",
      "was  |  AUX  |  auxiliary\n",
      "$  |  SYM  |  symbol\n",
      "18.8  |  NUM  |  numeral\n",
      "billion  |  NUM  |  numeral\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "increased  |  VERB  |  verb\n",
      "21  |  NUM  |  numeral\n",
      "%  |  NOUN  |  noun\n",
      "\n",
      "  |  SPACE  |  space\n",
      "·  |  PUNCT  |  punctuation\n",
      "          |  SPACE  |  space\n",
      "Diluted  |  VERB  |  verb\n",
      "earnings  |  NOUN  |  noun\n",
      "per  |  ADP  |  adposition\n",
      "share  |  NOUN  |  noun\n",
      "was  |  AUX  |  auxiliary\n",
      "$  |  SYM  |  symbol\n",
      "2.48  |  NUM  |  numeral\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "increased  |  VERB  |  verb\n",
      "22  |  NUM  |  numeral\n",
      "%  |  NOUN  |  noun\n",
      "\n",
      "  |  SPACE  |  space\n",
      "“  |  PUNCT  |  punctuation\n",
      "Digital  |  PROPN  |  proper noun\n",
      "technology  |  NOUN  |  noun\n",
      "etc  |  X  |  other\n",
      ".  |  X  |  other\n",
      "is  |  AUX  |  auxiliary\n",
      "the  |  DET  |  determiner\n",
      "most  |  ADV  |  adverb\n",
      "malleable  |  ADJ  |  adjective\n",
      "resource  |  NOUN  |  noun\n",
      "at  |  ADP  |  adposition\n",
      "the  |  DET  |  determiner\n",
      "world  |  NOUN  |  noun\n",
      "’s  |  PART  |  particle\n",
      "disposal  |  NOUN  |  noun\n",
      "to  |  PART  |  particle\n",
      "overcome  |  VERB  |  verb\n",
      "constraints  |  NOUN  |  noun\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "reimagine  |  VERB  |  verb\n",
      "everyday  |  ADJ  |  adjective\n",
      "work  |  NOUN  |  noun\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "life  |  NOUN  |  noun\n",
      ",  |  PUNCT  |  punctuation\n",
      "”  |  PUNCT  |  punctuation\n",
      "said  |  VERB  |  verb\n",
      "Satya  |  PROPN  |  proper noun\n",
      "Nadella  |  PROPN  |  proper noun\n",
      ",  |  PUNCT  |  punctuation\n",
      "chairman  |  NOUN  |  noun\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "chief  |  ADJ  |  adjective\n",
      "executive  |  ADJ  |  adjective\n",
      "officer  |  NOUN  |  noun\n",
      "of  |  ADP  |  adposition\n",
      "Microsoft  |  PROPN  |  proper noun\n",
      ".  |  PUNCT  |  punctuation\n",
      "“  |  PUNCT  |  punctuation\n",
      "As  |  ADV  |  adverb\n",
      "tech  |  NOUN  |  noun\n",
      "as  |  ADP  |  adposition\n",
      "a  |  DET  |  determiner\n",
      "percentage  |  NOUN  |  noun\n",
      "of  |  ADP  |  adposition\n",
      "global  |  ADJ  |  adjective\n",
      "GDP  |  PROPN  |  proper noun\n",
      "continues  |  VERB  |  verb\n",
      "to  |  PART  |  particle\n",
      "increase  |  VERB  |  verb\n",
      ",  |  PUNCT  |  punctuation\n",
      "we  |  PRON  |  pronoun\n",
      "are  |  AUX  |  auxiliary\n",
      "innovating  |  VERB  |  verb\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "investing  |  VERB  |  verb\n",
      "across  |  ADP  |  adposition\n",
      "diverse  |  ADJ  |  adjective\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "growing  |  VERB  |  verb\n",
      "markets  |  NOUN  |  noun\n",
      ",  |  PUNCT  |  punctuation\n",
      "with  |  ADP  |  adposition\n",
      "a  |  DET  |  determiner\n",
      "common  |  ADJ  |  adjective\n",
      "underlying  |  VERB  |  verb\n",
      "technology  |  NOUN  |  noun\n",
      "stack  |  NOUN  |  noun\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "an  |  DET  |  determiner\n",
      "operating  |  NOUN  |  noun\n",
      "model  |  NOUN  |  noun\n",
      "that  |  PRON  |  pronoun\n",
      "reinforces  |  VERB  |  verb\n",
      "a  |  DET  |  determiner\n",
      "common  |  ADJ  |  adjective\n",
      "strategy  |  NOUN  |  noun\n",
      ",  |  PUNCT  |  punctuation\n",
      "culture  |  NOUN  |  noun\n",
      ",  |  PUNCT  |  punctuation\n",
      "and  |  CCONJ  |  coordinating conjunction\n",
      "sense  |  NOUN  |  noun\n",
      "of  |  ADP  |  adposition\n",
      "purpose  |  NOUN  |  noun\n",
      ".  |  PUNCT  |  punctuation\n",
      "”  |  PUNCT  |  punctuation\n",
      "\n",
      "  |  SPACE  |  space\n",
      "“  |  PUNCT  |  punctuation\n",
      "Solid  |  ADJ  |  adjective\n",
      "commercial  |  ADJ  |  adjective\n",
      "execution  |  NOUN  |  noun\n",
      ",  |  PUNCT  |  punctuation\n",
      "represented  |  VERB  |  verb\n",
      "by  |  ADP  |  adposition\n",
      "strong  |  ADJ  |  adjective\n",
      "bookings  |  NOUN  |  noun\n",
      "growth  |  NOUN  |  noun\n",
      "driven  |  VERB  |  verb\n",
      "by  |  ADP  |  adposition\n",
      "long  |  ADJ  |  adjective\n",
      "-  |  PUNCT  |  punctuation\n",
      "term  |  NOUN  |  noun\n",
      "Azure  |  ADJ  |  adjective\n",
      "commitments  |  NOUN  |  noun\n",
      ",  |  PUNCT  |  punctuation\n",
      "increased  |  VERB  |  verb\n",
      "Microsoft  |  PROPN  |  proper noun\n",
      "Cl  |  PROPN  |  proper noun\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \",token.pos_, \" | \",spacy.explain(token.pos_))\n",
    "    \n",
    "    #based on some use case we need to remove punctuation,space and other symbols to perfom analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f18e5d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",  |  PUNCT  |  punctuation\n",
      ",  |  PUNCT  |  punctuation\n",
      ":  |  PUNCT  |  punctuation\n",
      "\n",
      "\n",
      "  |  SPACE  |  space\n",
      "·  |  PUNCT  |  punctuation\n",
      "          |  SPACE  |  space\n",
      "\n",
      "  |  SPACE  |  space\n",
      "·  |  PUNCT  |  punctuation\n",
      "          |  SPACE  |  space\n",
      "\n",
      "  |  SPACE  |  space\n",
      "·  |  PUNCT  |  punctuation\n",
      "          |  SPACE  |  space\n",
      "\n",
      "  |  SPACE  |  space\n",
      "·  |  PUNCT  |  punctuation\n",
      "          |  SPACE  |  space\n",
      "\n",
      "  |  SPACE  |  space\n",
      "“  |  PUNCT  |  punctuation\n",
      "etc  |  X  |  other\n",
      ".  |  X  |  other\n",
      ",  |  PUNCT  |  punctuation\n",
      "”  |  PUNCT  |  punctuation\n",
      ",  |  PUNCT  |  punctuation\n",
      ".  |  PUNCT  |  punctuation\n",
      "“  |  PUNCT  |  punctuation\n",
      ",  |  PUNCT  |  punctuation\n",
      ",  |  PUNCT  |  punctuation\n",
      ",  |  PUNCT  |  punctuation\n",
      ",  |  PUNCT  |  punctuation\n",
      ".  |  PUNCT  |  punctuation\n",
      "”  |  PUNCT  |  punctuation\n",
      "\n",
      "  |  SPACE  |  space\n",
      "“  |  PUNCT  |  punctuation\n",
      ",  |  PUNCT  |  punctuation\n",
      "-  |  PUNCT  |  punctuation\n",
      ",  |  PUNCT  |  punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ in [\"SPACE\", \"X\", \"PUNCT\"]:\n",
    "        print(token, \" | \",token.pos_, \" | \",spacy.explain(token.pos_))\n",
    "        \n",
    "        # these are un necessary character that need to bee cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b6223ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Microsoft,\n",
       " Corp.,\n",
       " today,\n",
       " announced,\n",
       " the,\n",
       " following,\n",
       " results,\n",
       " for,\n",
       " the,\n",
       " quarter,\n",
       " ended,\n",
       " December,\n",
       " 31,\n",
       " 2021,\n",
       " as,\n",
       " compared,\n",
       " to,\n",
       " the,\n",
       " corresponding,\n",
       " period,\n",
       " of,\n",
       " last,\n",
       " fiscal,\n",
       " year,\n",
       " Revenue,\n",
       " was,\n",
       " $,\n",
       " 51.7,\n",
       " billion,\n",
       " and,\n",
       " increased,\n",
       " 20,\n",
       " %,\n",
       " Operating,\n",
       " income,\n",
       " was,\n",
       " $,\n",
       " 22.2,\n",
       " billion,\n",
       " and,\n",
       " increased,\n",
       " 24,\n",
       " %,\n",
       " Net,\n",
       " income,\n",
       " was,\n",
       " $,\n",
       " 18.8,\n",
       " billion,\n",
       " and,\n",
       " increased,\n",
       " 21,\n",
       " %,\n",
       " Diluted,\n",
       " earnings,\n",
       " per,\n",
       " share,\n",
       " was,\n",
       " $,\n",
       " 2.48,\n",
       " and,\n",
       " increased,\n",
       " 22,\n",
       " %,\n",
       " Digital,\n",
       " technology,\n",
       " is,\n",
       " the,\n",
       " most,\n",
       " malleable,\n",
       " resource,\n",
       " at,\n",
       " the,\n",
       " world,\n",
       " ’s,\n",
       " disposal,\n",
       " to,\n",
       " overcome,\n",
       " constraints,\n",
       " and,\n",
       " reimagine,\n",
       " everyday,\n",
       " work,\n",
       " and,\n",
       " life,\n",
       " said,\n",
       " Satya,\n",
       " Nadella,\n",
       " chairman,\n",
       " and,\n",
       " chief,\n",
       " executive,\n",
       " officer,\n",
       " of,\n",
       " Microsoft,\n",
       " As,\n",
       " tech,\n",
       " as,\n",
       " a,\n",
       " percentage,\n",
       " of,\n",
       " global,\n",
       " GDP,\n",
       " continues,\n",
       " to,\n",
       " increase,\n",
       " we,\n",
       " are,\n",
       " innovating,\n",
       " and,\n",
       " investing,\n",
       " across,\n",
       " diverse,\n",
       " and,\n",
       " growing,\n",
       " markets,\n",
       " with,\n",
       " a,\n",
       " common,\n",
       " underlying,\n",
       " technology,\n",
       " stack,\n",
       " and,\n",
       " an,\n",
       " operating,\n",
       " model,\n",
       " that,\n",
       " reinforces,\n",
       " a,\n",
       " common,\n",
       " strategy,\n",
       " culture,\n",
       " and,\n",
       " sense,\n",
       " of,\n",
       " purpose,\n",
       " Solid,\n",
       " commercial,\n",
       " execution,\n",
       " represented,\n",
       " by,\n",
       " strong,\n",
       " bookings,\n",
       " growth,\n",
       " driven,\n",
       " by,\n",
       " long,\n",
       " term,\n",
       " Azure,\n",
       " commitments,\n",
       " increased,\n",
       " Microsoft,\n",
       " Cl]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_tok = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"X\", \"PUNCT\"]:\n",
    "        filter_tok.append(token)\n",
    "        \n",
    "\n",
    "filter_tok  \n",
    "\n",
    "# all is cleaned out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea27a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also know how many noun verb are their by using count_by API\n",
    "\n",
    "count = doc.count_by(spacy.attrs.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7598b36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN  -->  10\n",
      "NOUN  -->  39\n",
      "VERB  -->  23\n",
      "DET  -->  9\n",
      "ADP  -->  13\n",
      "NUM  -->  13\n",
      "PUNCT  -->  23\n",
      "SCONJ  -->  1\n",
      "ADJ  -->  17\n",
      "SPACE  -->  10\n",
      "AUX  -->  6\n",
      "SYM  -->  4\n",
      "CCONJ  -->  11\n",
      "X  -->  2\n",
      "ADV  -->  2\n",
      "PART  -->  3\n",
      "PRON  -->  2\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text,\" --> \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c17b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want only NOUN and NUM\n",
    "\n",
    "Noun_token = []\n",
    "Num_token = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        Noun_token.append(token)\n",
    "    elif token.pos_ == \"NUM\":\n",
    "        Num_token.append(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8aa79806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[today,\n",
       " results,\n",
       " quarter,\n",
       " period,\n",
       " year,\n",
       " Revenue,\n",
       " %,\n",
       " income,\n",
       " %,\n",
       " income,\n",
       " %,\n",
       " earnings,\n",
       " share,\n",
       " %,\n",
       " technology,\n",
       " resource,\n",
       " world,\n",
       " disposal,\n",
       " constraints,\n",
       " work,\n",
       " life,\n",
       " chairman,\n",
       " officer,\n",
       " tech,\n",
       " percentage,\n",
       " markets,\n",
       " technology,\n",
       " stack,\n",
       " operating,\n",
       " model,\n",
       " strategy,\n",
       " culture,\n",
       " sense,\n",
       " purpose,\n",
       " execution,\n",
       " bookings,\n",
       " growth,\n",
       " term,\n",
       " commitments]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Noun_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bdf937c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 2021, 51.7, billion, 20, 22.2, billion, 24, 18.8, billion, 21, 2.48, 22]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Num_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e268bfe",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb5a2f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# To look all stopwords in english model\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1f2f9851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STOP_WORDS)\n",
    "\n",
    "# their are 326 of them in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be3309d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "just\n",
      "our\n",
      "the\n",
      "part\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"We just opened our wings, the flying part is coming soon\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_stop:  #to see stop words\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f6a5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    no_stop_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "457dbd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opened', 'wings', 'flying', 'coming', 'soon']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"We just opened our wings, the flying part is coming soon\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
